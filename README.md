# ğŸ§  Human-Feedback-Safety-Simulator
 â€“ AI Safety Research Platform

A showcase-ready platform for aligning LLMs using RLHF, harm taxonomies, and Chain-of-Thought reasoning. Built with Streamlit for researchers, engineers, and policy teams working on LLM safety.

---

## ğŸš€ Key Features

* Define and manage **harm taxonomies**
* Generate and annotate **LLM outputs**
* Train **reward models** via preference learning
* Evaluate safety using custom metrics
* âœ… Supports Chain-of-Thought Verifiers
* âœ… Works in **simulation mode** (no PyTorch required)
* ğŸ§ª Research paper template + professional UI

---

## ğŸ› ï¸ System Overview

### ğŸ”§ Frontend

* `Streamlit` with multi-page layout
* Sidebar navigation, session state management

### ğŸ”¨ Backend Modules

* `TaxonomyManager`: harm categories
* `PromptGenerator`: create harmful/edge-case prompts
* `PreferenceTrainer`: pairwise ranking models
* `ChainOfThoughtVerifier`: CoT reasoning checks (ğŸ†•)
* `HarmEvaluator`, `ModelLoader`, `VisualizationManager`

---

## ğŸ“ Data Structure

Stored in JSON for portability:

```text
ğŸ“† data/
 â”œ taxonomy.json
 â”œ prompts.json
 â”œ raw_outputs.json
 â”œ aligned_outputs.json
 â”œ preferences.json
 â”œ training_results.json
 â”— verifications.json
```

Supports export in `.csv` or `.json`.

---

## ğŸ“‰ Evaluation & Metrics

* Severity scores (1â€“5)
* Harm frequency comparison (pre/post alignment)
* CoT failure case tracking
* Real-time safe/unsafe reasoning detection

---

## ğŸ“¦ Dependencies

| Area     | Libraries Used             |
| -------- | -------------------------- |
| LLM      | HuggingFace Transformers   |
| RLHF     | OpenRLHF / RLHFlow         |
| UI       | Streamlit, Plotly          |
| Data     | Pandas, NumPy, JSON        |
| Optional | CUDA, PyTorch, HF Datasets |

---

## ğŸ“¦ Installation (Replit or Local)

```bash
git clone https://github.com/your-username/rlhf-studio.git
cd rlhf-studio
pip install -r requirements.txt
streamlit run app.py
```

> Works in Replit with GPU/CPU detection and simulation mode for no-hardware environments.

---

## ğŸ“Š Showcase Ready

* Live demo workflows
* Structured experiment directories
* Collaboration-ready layout
* âœ… Publication support template included

---

## ğŸ“š Recent Updates (July 2025)

* âœ… CoT verifier integration
* âœ… Simulation mode (PyTorch-free)
* âœ… Professional UI for portfolios
* âœ… PyTorch-ready backend
* âœ… Policy alignment features added

---

## ğŸ§ª Ideal Use Cases

* RLHF alignment experiments
* LLM safety evaluations
* AI policy interface prototyping
* Academic and applied AI research

---

## ğŸŒ License

MIT License

---

## ğŸ™Œ Contribute

Pull requests, ideas, and RLHF memes welcome.


The platform is designed as a self-contained research environment that can be easily deployed and used for AI safety research without requiring complex infrastructure setup.
